import argparse
from datetime import datetime, timedelta
from PyPDF2 import PdfReader
import subprocess
import time
from langdetect import detect_langs, DetectorFactory
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import requests
import importlib
import shutil
import os
import json
from pathlib import Path
from text_utils import reflow_paragraphs

# Tool version for metadata
TOOL_VERSION = "0.2"

# Default output root directory (iCloud Obsidian Vault)
DEFAULT_OUTPUT_ROOT = os.path.expanduser("/Users/fjwrk/Library/Mobile Documents/iCloud~md~obsidian/Documents/den/BookShelf")

# è¨­å®š
PDF_PATH = None
MINUTES_PER_DAY = 20  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1æ—¥ã‚ãŸã‚Šã®èª­æ›¸æ™‚é–“ï¼ˆåˆ†ï¼‰
START_DATE = datetime.today()


# PDFãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆ
def get_pdf_page_texts_and_counts(pdf_path):
    reader = PdfReader(pdf_path)
    page_texts = []
    page_char_counts = []
    for page in reader.pages:
        text = page.extract_text() or ""
        page_texts.append(text)
        page_char_counts.append(len(text))
    return page_texts, page_char_counts


def detect_language_of_document(page_texts, sample_chars=2000):
    DetectorFactory.seed = 0
    sample = ""
    for t in page_texts:
        if t and t.strip():
            sample += t + "\n"
        if len(sample) >= sample_chars:
            break
    if not sample:
        return None
    try:
        langs = detect_langs(sample)
        if not langs:
            return None
        # æœ€ã‚‚ç¢ºåº¦ã®é«˜ã„è¨€èªã‚’è¿”ã™
        return langs[0].lang
    except Exception:
        return None


def normalize_paragraphs(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®è¡Œåˆ†å‰²ã‚’è§£é™¤ã—ã¦æ„å‘³ã®ã‚ã‚‹æ®µè½å˜ä½ã«æ•´å½¢ã™ã‚‹ã€‚

    - ç©ºè¡Œã¯æ®µè½å¢ƒç•Œã¨ã¿ãªã™
    - è¡Œæœ«ãŒãƒã‚¤ãƒ•ãƒ³ãªã‚‰ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤ã—ã¦å˜èªã‚’é€£çµ
    - ãã®ä»–ã¯ç©ºç™½ã§é€£çµã—ã¦æ®µè½ã‚’æ§‹æˆã™ã‚‹
    """
    if not text:
        return []
    text = text.replace("\r", "\n")
    lines = [ln.strip() for ln in text.split("\n")]
    paras = []
    cur = ""
    for line in lines:
        if not line:
            if cur:
                paras.append(cur.strip())
                cur = ""
            continue
        if not cur:
            cur = line
            continue
        # ãƒã‚¤ãƒ•ãƒ³ã«ã‚ˆã‚‹ç¶™ç¶šï¼ˆå˜èªåˆ†æ–­ï¼‰
        if cur.endswith("-"):
            cur = cur[:-1] + line
            continue
        # æ™®é€šã¯ã‚¹ãƒšãƒ¼ã‚¹ã§é€£çµï¼ˆå¤§ããªæ®µè½ã‚’ä½œã‚‹ï¼‰
        cur = cur + " " + line
    if cur:
        paras.append(cur.strip())
    return paras


def chunk_paragraphs(paragraphs, max_chars=800):
    """æ®µè½ãƒªã‚¹ãƒˆã‚’æŒ‡å®šæ–‡å­—æ•°ä»¥ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ï¼ˆæ®µè½ãƒªã‚¹ãƒˆï¼‰ã«åˆ†å‰²ã™ã‚‹ã€‚"""
    if not paragraphs:
        return []
    chunks = []
    cur = []
    cur_len = 0
    for p in paragraphs:
        plen = len(p)
        if cur_len + plen <= max_chars or not cur:
            cur.append(p)
            cur_len += plen
        else:
            chunks.append(cur)
            cur = [p]
            cur_len = plen
    if cur:
        chunks.append(cur)
    return chunks


def translate_pages_to_japanese(page_texts, out_path="translations.md"):
    # æ—§: googletrans ã‚’ä½¿ã£ãŸç¿»è¨³ï¼ˆéå…¬å¼ï¼‰ã€‚ã“ã“ã§ã¯ MarianMT ã‚’åˆ©ç”¨ã™ã‚‹é–¢æ•°ã‚’åˆ¥ã«ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚
    print("translate_pages_to_japanese: éæ¨å¥¨ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«MarianMTã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰")


def translate_pages_marian(
    page_texts,
    out_path="translations.md",
    model_id="Helsinki-NLP/opus-mt-en-ja",
    device=None,
):
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Loading model {model_id} on {device}...")
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)
    # Determine encoder max length from model config (fallback to 512)
    encoder_max_len = getattr(model.config, "max_position_embeddings", None)
    if encoder_max_len is None:
        encoder_max_len = getattr(model.config, "max_length", 512)
    try:
        encoder_max_len = int(encoder_max_len)
    except Exception:
        encoder_max_len = 512
    # cap to a reasonable upper bound
    encoder_max_len = min(encoder_max_len, 1024)

    translated_pages = ["" for _ in page_texts]
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("# Translations (Japanese)\n\n")
        for idx, text in enumerate(page_texts):
            if not text or not text.strip():
                continue
            # Use reflow_paragraphs to align translation units with display
            paras_text = reflow_paragraphs(text)
            paragraphs = paras_text.split("\n\n") if paras_text else []
            para_chunks = chunk_paragraphs(paragraphs, max_chars=800)

            translated_parts = []
            for chunk_paras in para_chunks:
                chunk_text = "\n\n".join(chunk_paras)
                try:
                    inputs = tokenizer(
                        chunk_text,
                        return_tensors="pt",
                        truncation=True,
                        max_length=encoder_max_len,
                    )
                    input_ids = inputs.get("input_ids")
                    attention_mask = inputs.get("attention_mask")
                    if input_ids is None:
                        raise ValueError("tokenizer returned no input_ids")
                    input_ids = input_ids.to(device)
                    if attention_mask is not None:
                        attention_mask = attention_mask.to(device)
                    try:
                        outs = model.generate(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            max_length=256,
                            num_beams=4,
                            early_stopping=True,
                        )
                    except IndexError as ie:
                        print(
                            f"ãƒšãƒ¼ã‚¸{idx + 1}ãƒãƒ£ãƒ³ã‚¯ã§IndexError: {ie}; input_ids.shape={tuple(input_ids.shape)}"
                        )
                        continue
                    except Exception:
                        try:
                            outs = model.generate(
                                input_ids=input_ids,
                                attention_mask=attention_mask,
                                max_length=128,
                                num_beams=2,
                                early_stopping=True,
                            )
                        except Exception as e2:
                            print(
                                f"ãƒšãƒ¼ã‚¸{idx + 1}ãƒãƒ£ãƒ³ã‚¯ã®ç”Ÿæˆã§å¤±æ•—: {e2} â€” å½“è©²ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                            )
                            continue
                    translated = tokenizer.decode(outs[0], skip_special_tokens=True)
                    translated_parts.append(translated)
                except Exception as e:
                    print(
                        f"ãƒšãƒ¼ã‚¸{idx + 1}ã®ãƒãƒ£ãƒ³ã‚¯ç¿»è¨³ã§ã‚¨ãƒ©ãƒ¼: {e} â€” å½“è©²ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                    )
                    continue
            full_trans = "\n\n".join(translated_parts)
            translated_pages[idx] = full_trans
            f.write(f"## Page {idx + 1}\n\n")
            f.write(full_trans + "\n\n")
    print(f"æ—¥æœ¬èªè¨³ã‚’{out_path}ã«å‡ºåŠ›ã—ã¾ã—ãŸï¼ˆãƒ¢ãƒ‡ãƒ«: {model_id}ï¼‰ã€‚")
    return translated_pages


# æŒ‡å®šãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³èª­ã—æ‰€è¦æ™‚é–“ã‚’è¨ˆæ¸¬ï¼ˆmacOS ã® say ã‚’åˆ©ç”¨ï¼‰
def tts_and_measure(text, say_rate=None):
    # say ã«ä¸ãˆã‚‹ã¨é•·ã™ãã¦å¤±æ•—ã™ã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€çŸ­ãåŒºåˆ‡ã£ã¦å®Ÿè¡Œ
    CHUNK = 4000
    start = time.time()
    cmd_base = ["say"]
    if say_rate:
        cmd_base += ["-r", str(int(say_rate))]
    for i in range(0, len(text), CHUNK):
        chunk = text[i : i + CHUNK]
        subprocess.run(cmd_base + [chunk])
    end = time.time()
    return end - start


# ã‚µãƒ³ãƒ—ãƒ«ãƒšãƒ¼ã‚¸ã§1æ–‡å­—ã‚ãŸã‚Šã®éŸ³èª­ç§’æ•°ã‚’è¨ˆæ¸¬
def measure_seconds_per_char(
    page_texts, sample_pages=None, say_rate=None, skip_tts=False
):
    # sample_pages ãŒæœªæŒ‡å®šãªã‚‰ã€æœ€åˆã®éç©ºãƒšãƒ¼ã‚¸ã‚’æœ€å¤§3ãƒšãƒ¼ã‚¸ã‚µãƒ³ãƒ—ãƒ«ã«ã™ã‚‹
    if sample_pages is None:
        sample_pages = []
        for idx, text in enumerate(page_texts):
            if text and text.strip():
                sample_pages.append(idx)
            if len(sample_pages) >= 3:
                break

    total_chars = 0
    total_seconds = 0.0
    for i in sample_pages:
        if i < 0 or i >= len(page_texts):
            continue
        text = page_texts[i]
        if not text or not text.strip():
            continue
        print(f"ãƒšãƒ¼ã‚¸{i + 1}ã‚’éŸ³èª­ã—ã¦è¨ˆæ¸¬ã—ã¾ã™...")
        if skip_tts:
            print("éŸ³èª­è¨ˆæ¸¬ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰ã€‚")
            sec = 0.0
        else:
            sec = tts_and_measure(text, say_rate=say_rate)
        chars = len(text)
        print(f"{chars}æ–‡å­—, {sec:.2f}ç§’")
        total_chars += chars
        total_seconds += sec

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ãŒç„¡ã„å ´åˆã‚„ç§’æ•°ãŒ0ã®å ´åˆã¯1æ–‡å­—ã‚ãŸã‚Š0.5ç§’
    if total_chars == 0 or total_seconds == 0.0:
        return 0.5
    return total_seconds / total_chars


# ãƒšãƒ¼ã‚¸ã”ã¨ã®éŸ³èª­æ‰€è¦ç§’æ•°ãƒªã‚¹ãƒˆä½œæˆ
def estimate_page_seconds(page_char_counts, sec_per_char):
    return [count * sec_per_char for count in page_char_counts]


# èª­æ›¸è¨ˆç”»ç”Ÿæˆï¼ˆãƒšãƒ¼ã‚¸ã”ã¨ã®æ‰€è¦æ™‚é–“ã‚’è€ƒæ…®ï¼‰
def generate_reading_plan_by_seconds(page_seconds, minutes_per_day, start_date):
    seconds_per_day = minutes_per_day * 60
    plan = []
    i = 0
    total_pages = len(page_seconds)
    while i < total_pages:
        day = start_date + timedelta(days=len(plan))
        start_page = i + 1
        acc_sec = 0.0
        # å¯èƒ½ãªé™ã‚Šãã®æ—¥ã®æ™‚é–“ã«åã¾ã‚‹ãƒšãƒ¼ã‚¸ã‚’è¿½åŠ 
        while i < total_pages and acc_sec + page_seconds[i] <= seconds_per_day:
            acc_sec += page_seconds[i]
            i += 1
        # 1ãƒšãƒ¼ã‚¸ã‚‚å…¥ã‚‰ãªã‹ã£ãŸå ´åˆã¯1ãƒšãƒ¼ã‚¸ã ã‘é€²ã‚ã‚‹ï¼ˆé•·ã„ç« ã®å ´åˆã‚’æƒ³å®šï¼‰
        if start_page == i + 1:
            i += 1
        end_page = i
        plan.append(
            {
                "date": day.strftime("%Y-%m-%d"),
                "start_page": start_page,
                "end_page": end_page,
            }
        )
    return plan


# Obsidianç”¨ToDoå‡ºåŠ›
def export_obsidian_todo(plan, pdf_basename=None, out_dir=None):
    # Tasks-style Markdown tasks with scheduled/due and absolute link to HTML page
    lines = []
    for item in plan:
        date = item["date"]
        start = item["start_page"]
        end = item["end_page"]
        scheduled = date
        due = date
        # Absolute path to the HTML page in pages/
        if out_dir:
            p = Path(os.path.join(out_dir, "pages", f"page{start}.html")).resolve()
        else:
            p = Path(os.path.join("pages", f"page{start}.html")).resolve()
        # Use file URI so Obsidian recognizes absolute local file links
        abs_link = p.as_uri()
        # Include BookStand and PDF basename in each task title
        title_prefix = "BookStand"
        if pdf_basename:
            title_prefix = f"{title_prefix} {pdf_basename}"
        lines.append(
            f"- [ ] {title_prefix} â€” Pages {start}â€“{end} ğŸ›« {scheduled} ğŸ“† {due} â€” [Open HTML]({abs_link})"
        )
    return "\n".join(lines)


def write_plan_with_frontmatter(
    plan, pdf_path, minutes_per_day, tts_speed, out_path="reading_plan.md", pdf_basename=None, out_dir=None
):
    created = datetime.now().strftime("%Y-%m-%d")
    front = [
        "---",
        f'title: "Reading Plan for {pdf_path}"',
        f"created: {created}",
        f'pdf: "{pdf_path}"',
        f"minutes_per_day: {minutes_per_day}",
        f"tts_speed: {tts_speed}",
        f"total_pages: {len(plan) and sum([item['end_page'] - item['start_page'] + 1 for item in plan]) or 0}",
        "---",
        "",
    ]
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(front))
        f.write(export_obsidian_todo(plan, pdf_basename=pdf_basename, out_dir=out_dir))
    print(f"Obsidianç”¨è¨ˆç”»ã‚’{out_path}ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate reading plan for a PDF with optional offline translation and TTS settings"
    )
    parser.add_argument("pdf", help="Path to PDF file")
    parser.add_argument("--minutes", type=int, default=20, help="Minutes per day")
    parser.add_argument(
        "--tts-speed",
        type=float,
        default=1.0,
        choices=[1.0, 1.5, 2.0],
        help="TTS playback speed multiplier",
    )
    parser.add_argument(
        "--no-measure",
        action="store_true",
        help="Skip TTS measurement and use fallback speed",
    )
    parser.add_argument(
        "--no-translate",
        action="store_true",
        help="Skip translation step (do not attempt MarianMT)",
    )
    parser.add_argument(
        "--model-id",
        default="staka/fugumt-en-ja",
        help="MarianMT model id for translation",
    )
    parser.add_argument(
        "--translate-url",
        default=None,
        help="URL of a translation server (POST /translate) to use instead of local MarianMT",
    )
    parser.add_argument(
        "--export-html",
        action="store_true",
        help="Export per-page HTML viewer after generating translations",
    )
    parser.add_argument(
        "--html-outdir",
        default="html_output",
        help="Output directory for generated HTML (used with --export-html)",
    )
    parser.add_argument(
        "--out-root",
        default=DEFAULT_OUTPUT_ROOT,
        help="Root output directory for all generated files (per-PDF subfolders)",
    )
    args = parser.parse_args()

    PDF_PATH = args.pdf
    MINUTES_PER_DAY = args.minutes
    START_DATE = datetime.today()

    page_texts, page_char_counts = get_pdf_page_texts_and_counts(PDF_PATH)
    print(f"å…¨{len(page_texts)}ãƒšãƒ¼ã‚¸ã€ã‚µãƒ³ãƒ—ãƒ«ãƒšãƒ¼ã‚¸ã§éŸ³èª­é€Ÿåº¦ã‚’è¨ˆæ¸¬ã—ã¾ã™ã€‚")
    # map tts-speed multiplier to say rate (words per minute)
    default_wpm = 200
    say_rate = int(default_wpm * args.tts_speed)
    sec_per_char = measure_seconds_per_char(
        page_texts, say_rate=say_rate, skip_tts=args.no_measure
    )
    print(f"1æ–‡å­—ã‚ãŸã‚Šå¹³å‡{sec_per_char:.3f}ç§’ (tts-speed={args.tts_speed}x)")
    page_seconds = estimate_page_seconds(page_char_counts, sec_per_char)
    plan = generate_reading_plan_by_seconds(page_seconds, MINUTES_PER_DAY, START_DATE)

    # Prepare output directory: per-PDF subfolder under out-root with timestamp
    out_root = os.path.expanduser(args.out_root or DEFAULT_OUTPUT_ROOT)
    pdf_basename = os.path.splitext(os.path.basename(PDF_PATH))[0]
    ts = datetime.now().strftime("%Y%m%d%H%M%S")
    out_dir = os.path.join(out_root, f"{pdf_basename}_{ts}")
    staging_dir = out_dir + ".staging"
    # Ensure staging dir is clean
    if os.path.exists(staging_dir):
        shutil.rmtree(staging_dir)
    os.makedirs(staging_dir, exist_ok=True)

    # write Obsidian plan with frontmatter into staging_dir (reading_plan.md stays at root of staging)
    plan_path = os.path.join(staging_dir, "reading_plan.md")
    write_plan_with_frontmatter(
        plan,
        PDF_PATH,
        MINUTES_PER_DAY,
        args.tts_speed,
        out_path=plan_path,
        pdf_basename=pdf_basename,
        out_dir=out_dir,
    )
    # create md subfolder for translations and segments inside staging
    md_dir = os.path.join(staging_dir, "md")
    os.makedirs(md_dir, exist_ok=True)
    # è‡ªå‹•è¨€èªåˆ¤å®šã¨ï¼ˆè‹±èªãªã‚‰ï¼‰MarianMTã«ã‚ˆã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç¿»è¨³
    if args.no_translate:
        translated_pages = None
        print("ç¿»è¨³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆ--no-translateï¼‰ã€‚")
    else:
        lang = detect_language_of_document(page_texts)
        print(f"æ¤œå‡ºè¨€èª: {lang}")
        if lang and lang.startswith("en"):
            if args.translate_url:
                try:
                    def translate_pages_remote(page_texts, out_path="translations.md", translate_url=None, src_lang="en_XX", tgt_lang="ja_XX"):
                        # translate_url must be provided when using remote translation
                        if translate_url is None:
                            raise ValueError("translate_url must be provided for remote translation")
                        translated_pages = ["" for _ in page_texts]
                        with open(out_path, "w", encoding="utf-8") as f:
                            f.write("# Translations (Japanese)\n\n")
                            for idx, text in enumerate(page_texts):
                                if not text or not text.strip():
                                    continue
                                # Use reflow_paragraphs so translation units match display
                                paras_text = reflow_paragraphs(text)
                                paragraphs = paras_text.split("\n\n") if paras_text else []
                                para_chunks = chunk_paragraphs(paragraphs, max_chars=800)
                                translated_parts = []
                                for chunk_paras in para_chunks:
                                    chunk_text = "\n\n".join(chunk_paras)
                                    try:
                                        resp = requests.post(
                                            translate_url,
                                            json={"text": chunk_text, "src_lang": src_lang, "tgt_lang": tgt_lang},
                                            timeout=30,
                                        )
                                        if resp.status_code != 200:
                                            print(f"ãƒšãƒ¼ã‚¸{idx+1}ãƒãƒ£ãƒ³ã‚¯ç¿»è¨³ã§ã‚µãƒ¼ãƒå¿œç­”ã‚¨ãƒ©ãƒ¼: {resp.status_code}")
                                            continue
                                        data = resp.json()
                                        translated_parts.append(data.get("text", ""))
                                    except Exception as e:
                                        print(f"ãƒšãƒ¼ã‚¸{idx+1}ãƒãƒ£ãƒ³ã‚¯ã®ãƒªãƒ¢ãƒ¼ãƒˆç¿»è¨³å¤±æ•—: {e}")
                                        continue
                                full_trans = "\n\n".join(translated_parts)
                                translated_pages[idx] = full_trans
                                f.write(f"## Page {idx + 1}\n\n")
                                f.write(full_trans + "\n\n")
                        print(f"æ—¥æœ¬èªè¨³ã‚’{out_path}ã«å‡ºåŠ›ã—ã¾ã—ãŸï¼ˆremote: {translate_url}ï¼‰ã€‚")
                        return translated_pages

                    translations_path = os.path.join(md_dir, "translations.md")
                    translated_pages = translate_pages_remote(
                        page_texts, out_path=translations_path, translate_url=args.translate_url
                    )
                except Exception as e:
                    print("ãƒªãƒ¢ãƒ¼ãƒˆç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
                    translated_pages = None
            else:
                try:
                    translations_path = os.path.join(md_dir, "translations.md")
                    translated_pages = translate_pages_marian(
                        page_texts, out_path=translations_path, model_id=args.model_id
                    )
                except Exception as e:
                    print("ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
                    translated_pages = None
        else:
            translated_pages = None
            print(
                "è‹±èªæ–‡æ›¸ã§ã¯ãªã„ã‹åˆ¤å®šã§ããŸãŸã‚ã€ç¿»è¨³ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚å¿…è¦ãªã‚‰å¼·åˆ¶ç¿»è¨³ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )

    # å„æ—¥ã®èª­ã¿ä¸Šã’å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã‚’ Markdown ã«å‡ºåŠ›
    def export_reading_segments(
        plan, page_texts, translated_pages=None, out_path="reading_segments.md"
    ):
        with open(out_path, "w", encoding="utf-8") as f:
            f.write("# Reading Segments\n\n")
            for item in plan:
                start = item["start_page"] - 1
                end = item["end_page"]
                if start >= end:
                    continue
                f.write(
                    f"## {item['date']} â€” Pages {item['start_page']}â€“{item['end_page']}\n\n"
                )
                # åŸæ–‡
                f.write("### Original\n\n")
                segment_text = "\n\n".join(page_texts[start:end])
                f.write(segment_text + "\n\n")
                # ç¿»è¨³ãŒã‚ã‚Œã°æ›¸ã
                if translated_pages:
                    f.write("### Japanese Translation\n\n")
                    trans_segment = "\n\n".join(
                        [
                            translated_pages[i]
                            for i in range(start, end)
                            if translated_pages[i]
                        ]
                    )
                    f.write(trans_segment + "\n\n")
                f.write("---\n\n")
        print(f"èª­ã¿ä¸Šã’ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’{out_path}ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")

    segments_path = os.path.join(md_dir, "reading_segments.md")
    export_reading_segments(plan, page_texts, translated_pages=translated_pages, out_path=segments_path)

    # Optional: export HTML viewer (PDF embed + original + translation)
    if args.export_html:
        try:
            # import export_html lazily
            export_html = importlib.import_module("export_html")
            # Generate HTML into staging_dir (PDF will be copied to final out_dir after staging complete)
            html_outdir = staging_dir  # write index.html and pages/pageN.html at staging_dir root
            print(f"Exporting HTML to {html_outdir} ...")
            translations_md = os.path.join(md_dir, "translations.md")
            reading_plan_md = os.path.join(staging_dir, "reading_plan.md")
            # Pass original PDF_PATH to exporter; generated pages reference ../<pdf_name> and will work after final copy
            export_html.generate(PDF_PATH, translations_md, reading_plan_md, out_dir=html_outdir)
            print("HTML export complete.")
        except Exception as e:
            print("HTMLã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ:", e)

    # Write metadata.json summarizing outputs in staging
    try:
        meta = {
            "tool_version": TOOL_VERSION,
            "generated_at": datetime.now().isoformat(),
            "pdf": os.path.abspath(PDF_PATH),
            "out_dir": os.path.abspath(out_dir),
            "minutes_per_day": MINUTES_PER_DAY,
            "tts_speed": args.tts_speed,
            "pages": len(page_texts),
            "translate_used": not args.no_translate,
            "translate_method": args.translate_url if args.translate_url else args.model_id,
            "generated_files": [
                os.path.join(staging_dir, "reading_plan.md"),
                os.path.join(staging_dir, "md", "translations.md"),
                os.path.join(staging_dir, "md", "reading_segments.md"),
                os.path.join(staging_dir, "pages"),
            ],
        }
        meta_path = os.path.join(staging_dir, "metadata.json")
        with open(meta_path, "w", encoding="utf-8") as mf:
            json.dump(meta, mf, ensure_ascii=False, indent=2)
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’{meta_path}ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)

    # Finalize: move staging to final out_dir atomically and copy PDF into out_dir
    try:
        if os.path.exists(out_dir):
            bak = out_dir + ".bak." + ts
            os.rename(out_dir, bak)
        os.rename(staging_dir, out_dir)
        # copy PDF into final out_dir
        try:
            shutil.copy2(PDF_PATH, out_dir)
        except Exception:
            pass
        print(f"æœ€çµ‚å‡ºåŠ›ã‚’{out_dir}ã«ç§»å‹•ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print("å‡ºåŠ›ã®æœ€çµ‚ç§»å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
